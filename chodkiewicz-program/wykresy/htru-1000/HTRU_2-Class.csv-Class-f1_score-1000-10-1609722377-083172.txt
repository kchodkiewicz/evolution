[9, 12, 29, 31, 35, 43, 46, 56, 57, 65]
Score: 0.9751707014276847 in 32 iterations
              precision    recall  f1-score   support

           0       0.98      0.99      0.99      1461
           1       0.89      0.83      0.86       150

    accuracy                           0.98      1611
   macro avg       0.94      0.91      0.92      1611
weighted avg       0.97      0.98      0.97      1611

Separate scores: [0.9757914338919925, 0.9689633767846059, 0.9658597144630664, 0.9695841092489137, 0.9695841092489137, 0.9689633767846059, 0.9726877715704532, 0.9621353196772191, 0.9726877715704532, 0.9615145872129114]
Random committee (for comparison): 0.9770328988206083
              precision    recall  f1-score   support

           0       0.98      0.99      0.99      1461
           1       0.93      0.81      0.87       150

    accuracy                           0.98      1611
   macro avg       0.96      0.90      0.93      1611
weighted avg       0.98      0.98      0.98      1611


 ------------------------------------------ 
1 & DecisionTreeClassifier & \multicolumn{1}{p{8cm}}{\texttt{(splitter=`random', max\_features=`sqrt')} }\\
2 & GaussianNB & \multicolumn{1}{p{8cm}}{\texttt{(var\_smoothing=0)} }\\
3 & KNeighborsClassifier & \multicolumn{1}{p{8cm}}{\texttt{()} }\\
4 & KNeighborsClassifier & \multicolumn{1}{p{8cm}}{\texttt{(n\_neighbors=10)} }\\
5 & KNeighborsClassifier & \multicolumn{1}{p{8cm}}{\texttt{(algorithm=`brute')} }\\
6 & LogisticRegression & \multicolumn{1}{p{8cm}}{\texttt{(solver=`saga', penalty=`l1')} }\\
7 & LogisticRegression & \multicolumn{1}{p{8cm}}{\texttt{(solver=`liblinear', penalty=`l2')} }\\
8 & QuadraticDiscriminantAnalysis & \multicolumn{1}{p{8cm}}{\text{()} }\\
9 & QuadraticDiscriminantAnalysis & \multicolumn{1}{p{8cm}}{\texttt{(store\_covariance=True)} }\\
10 & LinearDiscriminantAnalysis & \multicolumn{1}{p{8cm}}{\texttt{(solver=`lsqr', shrinkage=1.0)} }\\
