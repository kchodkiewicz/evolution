[7, 28, 34, 36, 46, 48, 58, 71, 77, 83]
Score: 0.7142857142857143 in 21 iterations
              precision    recall  f1-score   support

           0       0.64      0.75      0.69        12
           1       0.79      0.69      0.73        16

    accuracy                           0.71        28
   macro avg       0.71      0.72      0.71        28
weighted avg       0.72      0.71      0.72        28

Separate scores: [0.75, 0.7857142857142857, 0.8214285714285714, 0.7857142857142857, 0.8928571428571429, 0.7857142857142857, 0.7857142857142857, 0.8214285714285714, 0.8214285714285714, 0.8928571428571429]
Random committee (for comparison): 0.8214285714285714
              precision    recall  f1-score   support

           0       0.82      0.75      0.78        12
           1       0.82      0.88      0.85        16

    accuracy                           0.82        28
   macro avg       0.82      0.81      0.82        28
weighted avg       0.82      0.82      0.82        28


------------------------------------------
Chosen classifiers:
DecisionTreeClassifier(splitter='random', max_features='log2', )
SGDClassifier(loss='perceptron')
KNeighborsClassifier()
KNeighborsClassifier(n_neighbors=10)
GaussianProcessClassifier(n_restarts_optimizer=1)
GaussianProcessClassifier(max_iter_predict=10)
LogisticRegression(solver='saga', penalty='l1')
QuadraticDiscriminantAnalysis()
LinearDiscriminantAnalysis(n_components=1)
LinearDiscriminantAnalysis(solver='eigen', shrinkage=1.0)
