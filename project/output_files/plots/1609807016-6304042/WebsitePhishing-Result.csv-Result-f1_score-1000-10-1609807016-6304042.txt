[8, 15, 17, 24, 50, 51, 56, 65, 70, 74, 75]
Score: 0.8688524590163934 in 42 iterations
              precision    recall  f1-score   support

          -1       0.88      0.92      0.90        61
           0       1.00      0.17      0.29         6
           1       0.86      0.89      0.88        55

    accuracy                           0.87       122
   macro avg       0.91      0.66      0.69       122
weighted avg       0.87      0.87      0.86       122

Separate scores: [0.8278688524590164, 0.8852459016393442, 0.8770491803278688, 0.8852459016393442, 0.8934426229508197, 0.8852459016393442, 0.8770491803278688, 0.9016393442622952, 0.8852459016393442, 0.8852459016393442, 0.8934426229508197]
Random committee (for comparison): 0.9016393442622952
              precision    recall  f1-score   support

          -1       0.92      0.93      0.93        61
           0       1.00      0.17      0.29         6
           1       0.88      0.95      0.91        55

    accuracy                           0.90       122
   macro avg       0.93      0.68      0.71       122
weighted avg       0.91      0.90      0.89       122


 ------------------------------------------ 
Chosen classifiers:
DecisionTreeClassifier(criterion='entropy', max_features='sqrt')
SVC(gamma='auto')
SVC(C=1e2)
SGDClassifier()
GaussianProcessClassifier(n_restarts_optimizer=1, max_iter_predict=10)
GaussianProcessClassifier(n_restarts_optimizer=1, warm_start=True, max_iter_predict=10)
LogisticRegression(solver='liblinear', penalty='l1')
PassiveAggressiveClassifier(loss='squared_hinge')
PassiveAggressiveClassifier(n_iter_no_change=1)
QuadraticDiscriminantAnalysis(tol=1.0e-1, store_covariance=True)
LinearDiscriminantAnalysis()
